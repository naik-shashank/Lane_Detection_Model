{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-GQ6LVXU74v",
        "outputId": "333807d6-97b4-47a3-fe8a-659f86473858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 81ms/step - loss: 0.2887 - val_loss: 0.0999\n",
            "Epoch 2/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 56ms/step - loss: 0.0738 - val_loss: 0.0699\n",
            "Epoch 3/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - loss: 0.0560 - val_loss: 0.0477\n",
            "Epoch 4/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - loss: 0.0466 - val_loss: 0.0416\n",
            "Epoch 5/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0410 - val_loss: 0.0389\n",
            "Epoch 6/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 59ms/step - loss: 0.0391 - val_loss: 0.0377\n",
            "Epoch 7/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0368 - val_loss: 0.0364\n",
            "Epoch 8/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - loss: 0.0364 - val_loss: 0.0356\n",
            "Epoch 9/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0342 - val_loss: 0.0323\n",
            "Epoch 10/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 59ms/step - loss: 0.0309 - val_loss: 0.0485\n",
            "Epoch 11/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0363 - val_loss: 0.0301\n",
            "Epoch 12/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - loss: 0.0302 - val_loss: 0.0302\n",
            "Epoch 13/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - loss: 0.0292 - val_loss: 0.0298\n",
            "Epoch 14/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - loss: 0.0281 - val_loss: 0.0286\n",
            "Epoch 15/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0266 - val_loss: 0.0539\n",
            "Epoch 16/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0410 - val_loss: 0.0328\n",
            "Epoch 17/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - loss: 0.0268 - val_loss: 0.0280\n",
            "Epoch 18/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 0.0259 - val_loss: 0.0286\n",
            "Epoch 19/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - loss: 0.0265 - val_loss: 0.0291\n",
            "Epoch 20/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0257 - val_loss: 0.0265\n",
            "Epoch 21/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - loss: 0.0243 - val_loss: 0.0267\n",
            "Epoch 22/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 0.0242 - val_loss: 0.0260\n",
            "Epoch 23/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0240 - val_loss: 0.0252\n",
            "Epoch 24/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - loss: 0.0248 - val_loss: 0.0241\n",
            "Epoch 25/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - loss: 0.0229 - val_loss: 0.0252\n",
            "Epoch 26/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 62ms/step - loss: 0.0227 - val_loss: 0.0281\n",
            "Epoch 27/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - loss: 0.0277 - val_loss: 0.0252\n",
            "Epoch 28/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - loss: 0.0225 - val_loss: 0.0310\n",
            "Epoch 29/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - loss: 0.0232 - val_loss: 0.0256\n",
            "Epoch 30/30\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - loss: 0.0220 - val_loss: 0.0236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp187ixxgm'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 80, 160, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 80, 160, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137824935276240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824935277584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824935280272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824935279120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824935279312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824935278160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824935276816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824935280080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824935278736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928597584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928597776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928599312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928598544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928600656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928600080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928601424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928600464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137824928602576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "✅ TFLite model saved as 'lane_model.tflite'\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_data():\n",
        "    with open(\"/content/drive/MyDrive/full_CNN_train.p\", \"rb\") as f:\n",
        "        train_images = pickle.load(f)\n",
        "    with open(\"/content/drive/MyDrive/full_CNN_labels.p\", \"rb\") as f:\n",
        "        labels = pickle.load(f)\n",
        "\n",
        "    X = np.array(train_images, dtype=np.uint8)\n",
        "    y = np.array(labels, dtype=np.uint8)\n",
        "\n",
        "    # Normalize\n",
        "    X = X.astype(\"float32\") / 255.0\n",
        "    y = y.astype(\"float32\") / 255.0\n",
        "\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LaneNet model in Keras\n",
        "def build_model():\n",
        "    inputs = layers.Input(shape=(80, 160, 3))  # Adjust input size if needed\n",
        "\n",
        "    x1 = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
        "    p1 = layers.MaxPooling2D()(x1)\n",
        "\n",
        "    x2 = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(p1)\n",
        "    p2 = layers.MaxPooling2D()(x2)\n",
        "\n",
        "    x3 = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(p2)\n",
        "\n",
        "    u1 = layers.Conv2DTranspose(64, 2, strides=2, padding=\"same\")(x3)\n",
        "    c1 = layers.Concatenate()([u1, x2])\n",
        "    x4 = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(c1)\n",
        "\n",
        "    u2 = layers.Conv2DTranspose(32, 2, strides=2, padding=\"same\")(x4)\n",
        "    c2 = layers.Concatenate()([u2, x1])\n",
        "    x5 = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(c2)\n",
        "\n",
        "    x6 = layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(x5)\n",
        "    output = layers.Conv2D(1, 1, padding=\"same\")(x6)  # No sigmoid (use loss)\n",
        "\n",
        "    return models.Model(inputs, output)\n",
        "\n",
        "# Train the model\n",
        "def train():\n",
        "    X_train, X_val, y_train, y_val = load_data()\n",
        "\n",
        "    model = build_model()\n",
        "    model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
        "\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32)\n",
        "    model.save(\"lane_model.h5\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Convert to TFLite\n",
        "def convert_to_tflite(model):\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    with open(\"lane_model.tflite\", \"wb\") as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    print(\"✅ TFLite model saved as 'lane_model.tflite'\")\n",
        "\n",
        "# Run everything\n",
        "model = train()\n",
        "convert_to_tflite(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Load TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"lane_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_height = input_details[0]['shape'][1]\n",
        "input_width = input_details[0]['shape'][2]\n",
        "\n",
        "class LaneDetector:\n",
        "    def __init__(self):\n",
        "        self.recent_preds = []\n",
        "        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "\n",
        "    def preprocess(self, frame):\n",
        "        resized = cv2.resize(frame, (input_width, input_height))\n",
        "        norm = resized.astype(np.float32) / 255.0\n",
        "        return np.expand_dims(norm, axis=0)\n",
        "\n",
        "    def postprocess(self, output, orig_shape):\n",
        "        pred = output.squeeze()\n",
        "        pred = 1 / (1 + np.exp(-pred))  # Apply sigmoid manually\n",
        "        pred = (pred * 255).astype(np.uint8)\n",
        "        pred = cv2.morphologyEx(pred, cv2.MORPH_OPEN, self.kernel)\n",
        "        pred = cv2.GaussianBlur(pred, (5, 5), 0)\n",
        "\n",
        "        _, mask = cv2.threshold(pred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        clean_mask = np.zeros_like(mask)\n",
        "        for cnt in contours:\n",
        "            if cv2.contourArea(cnt) > 100:\n",
        "                cv2.drawContours(clean_mask, [cnt], -1, 255, -1)\n",
        "\n",
        "        # Temporal smoothing\n",
        "        self.recent_preds.append(clean_mask)\n",
        "        if len(self.recent_preds) > 3:\n",
        "            self.recent_preds.pop(0)\n",
        "        avg_mask = np.mean(self.recent_preds, axis=0).astype(np.uint8)\n",
        "\n",
        "        # Resize to original size\n",
        "        overlay = np.zeros((input_height, input_width, 3), dtype=np.uint8)\n",
        "        overlay[..., 1] = avg_mask  # Green channel\n",
        "\n",
        "        overlay = cv2.resize(overlay, (orig_shape[1], orig_shape[0]), interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "        return overlay\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        orig_shape = frame.shape\n",
        "        input_tensor = self.preprocess(frame)\n",
        "\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        overlay = self.postprocess(output, orig_shape)\n",
        "\n",
        "        # Limit overlay to road area (optional)\n",
        "        road_mask = cv2.inRange(cv2.cvtColor(frame, cv2.COLOR_BGR2HSV),\n",
        "                                (0, 0, 100), (180, 50, 255))\n",
        "        overlay[road_mask == 0] = 0\n",
        "\n",
        "        return cv2.addWeighted(frame, 0.8, overlay, 0.4, 0)\n",
        "\n",
        "def process_video(input_path, output_path):\n",
        "    detector = LaneDetector()\n",
        "    clip = VideoFileClip(input_path)\n",
        "    processed_clip = clip.fl_image(detector.process_frame)\n",
        "    processed_clip.write_videofile(output_path, audio=False, codec='libx264')\n",
        "\n",
        "# Example usage\n",
        "process_video(\"/content/drive/MyDrive/lanes_clip.mp4\", \"lanes_output_tflite.mp4\")\n",
        "print(\"✅ TFLite Inference complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa2Q_FcdVO2-",
        "outputId": "60667115-c0bb-4832-b6ee-c6e3fef1486b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-2-c6f2b8e82ee3>:27: RuntimeWarning: overflow encountered in exp\n",
            "  pred = 1 / (1 + np.exp(-pred))  # Apply sigmoid manually\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video lanes_output_tflite.mp4.\n",
            "Moviepy - Writing video lanes_output_tflite.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready lanes_output_tflite.mp4\n",
            "✅ TFLite Inference complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNCe4eKLX7r9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}